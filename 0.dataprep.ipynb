{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rawdf = pd.read_csv('data/raw_data_macrozoobenthos_1980-2005.csv', header=0)\n",
    "# rawdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat raw data into site by abundance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ttran\\AppData\\Local\\Temp\\ipykernel_26224\\482386081.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['site'] = 'site_' + data['decimallatitude'].astype(str) + '_' + data['decimallongitude'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Extract relevant columns to create Site by Abundance matrix\n",
    "data = rawdf[['scientificnameaccepted', \\\n",
    "    'decimallatitude', 'decimallongitude', \\\n",
    "    'yearcollected',  'monthcollected', 'daycollected', \\\n",
    "     'minimumdepthinmeters', 'maximumdepthinmeters', 'Count (Dmnless)'\n",
    "    ]]\n",
    "\n",
    "# Create variable Site using latitude and longitude\n",
    "data['site'] = 'site_' + data['decimallatitude'].astype(str) + '_' + data['decimallongitude'].astype(str)\n",
    "data = data[['site', 'scientificnameaccepted', \\\n",
    "    'decimallatitude', 'decimallongitude', \\\n",
    "    'yearcollected',  'monthcollected', 'daycollected', \\\n",
    "     'minimumdepthinmeters', 'maximumdepthinmeters', 'Count (Dmnless)'\n",
    "    ]]\n",
    "\n",
    "# Transform data to Site by Abundance matrix\n",
    "## --- agregated over date, month, year\n",
    "# data = data.groupby(['site', 'scientificnameaccepted'])['Count (Dmnless)'].sum().reset_index()\n",
    "\n",
    "# # Transform data to Site by Abundance matrix\n",
    "# data = data.pivot(index='site', columns='scientificnameaccepted', values='Count (Dmnless)')\n",
    "# data.fillna(0, inplace=True\n",
    "\n",
    "# data.to_csv('data/sps_macrozoobenthos_allyear.csv')\n",
    "\n",
    "## ----- Now we will create a matrix over time\n",
    "# data = data.groupby(['site', 'scientificnameaccepted', 'yearcollected', 'monthcollected', 'daycollected'])['Count (Dmnless)'].sum().reset_index()\n",
    "# data[\"MMDDYYY\"] = data[\"monthcollected\"].astype(str) + '-' + data[\"daycollected\"].astype(str) + '-' + data[\"yearcollected\"].astype(str)\n",
    "# # data[['site', 'MMDDYYY', 'scientificnameaccepted', 'Count (Dmnless)']].to_csv('data/sps_macrozoobenthos_timeseries.csv')\n",
    "\n",
    "# # Create column quarter of the year\n",
    "# data['quarter'] = pd.to_datetime(data['MMDDYYY']).dt.quarter\n",
    "# # Aggregate over quarter\n",
    "# data_quarter = data.groupby(['site', 'scientificnameaccepted', 'quarter', \"yearcollected\"])['Count (Dmnless)'].sum().reset_index()\n",
    "# data_quarter.to_csv('data/sps_macrozoobenthos_timeseries_quarter.csv')\n",
    "\n",
    "# # Aggregate over year\n",
    "# data_year = data.groupby(['site', 'scientificnameaccepted', \"yearcollected\"])['Count (Dmnless)'].sum().reset_index()\n",
    "# data_year.to_csv('data/sps_macrozoobenthos_timeseries_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environemental data SUV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the script below, I determined that the datasets variables are\n",
    "- SUR contains 'VARIABLES', 'Depth', 'Temperatur', 'Salinity', 'xCO2atm', 'Latitude', 'Longitude', 'JulianDay'\n",
    "- UOR contains 'VARIABLES', 'Depth', 'Temperatur', 'Salinity', 'Chlorophyl', 'Latitude', 'Longitude', 'JulianDay'\n",
    "- PFL, XBT does not contain extra necessary data more than : 'VARIABLES', 'Depth', 'Temperatur', 'Salinity'\n",
    "- CTD contains 'VARIABLES', 'Depth', 'Temperatur', 'Salinity', 'Oxygen', 'Chlorophyl'\n",
    "- OSD contains 'VARIABLES', 'Depth', 'Temperatur', 'Salinity', 'Oxygen', 'Phosphate', 'Silicate', 'Nitrate', 'Alkalinity', 'tCO2', 'Helium', 'Neon', 'CFC11', 'CFC12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ocldb1738015069.1150601.OSD.csv', mode=\"r\") as file:\n",
    "    filedata = file.readlines()\n",
    "# max_L, L = 0, []\n",
    "# for l in filedata:\n",
    "#     if \"VARIABLES\" in l:\n",
    "#         # print(list(map(lambda x: x.strip(), l.split(\",\"))))\n",
    "#         if max_L <= len(l.split(\",\")):\n",
    "#             max_L = len(l.split(\",\"))\n",
    "#             L = []\n",
    "#             for i in list(map(lambda x: x.strip(), l.split(\",\"))):\n",
    "#                 if len(i) >1:\n",
    "#                     L.append(i)\n",
    "                    \n",
    "# Check how the variables are ordered\n",
    "# for l in filedata:\n",
    "#     if \"VARIABLES\" in l:\n",
    "#         print(list(map(lambda x: x.strip(), l.split(\",\"))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes to extract data from raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the file\n",
    "# file_path = \"data/ocldb1738015069.1150601.OSD.csv\"\n",
    "# file_path = \"data/ocldb1738015069.1150601.CTD.csv\"\n",
    "file_path = \"data/ocldb1738015069.1150601.SUR.csv\"\n",
    "\n",
    "# Function to preprocess the data\n",
    "def prep_data(file_path, save=True):\n",
    "    # Read the file as a list of lines\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize variables\n",
    "    data = []\n",
    "    current_record = {}\n",
    "    depth_data = []\n",
    "    variable_names = []\n",
    "\n",
    "    # Pattern matching for the header fields\n",
    "    patterns = {\n",
    "        \"CAST\": r\"CAST\\s+,,\\s*(\\d+)\",\n",
    "        \"Year\": r\"Year\\s+,,\\s*(\\d+)\",\n",
    "        \"Month\": r\"Month\\s+,,\\s*(\\d+)\",\n",
    "        \"Day\": r\"Day\\s+,,\\s*(\\d+)\",\n",
    "        \"Longitude\": r\"Longitude\\s+,,\\s*([\\d.]+)\",\n",
    "        \"Latitude\": r\"Latitude\\s+,,\\s*([\\d.]+)\",\n",
    "    }\n",
    "\n",
    "    # Process each line (after VARIABLES section)\n",
    "    for line in lines:\n",
    "        # Extract metadata\n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                current_record[key] = float(match.group(1)) if \".\" in match.group(1) else int(match.group(1))\n",
    "\n",
    "        # Detect the start of variables section\n",
    "        if line.startswith(\"VARIABLES\"):\n",
    "            depth_data = []  # Reset depth data\n",
    "            variable_names = [v.strip() for v in line.split(\",\")[1:] if v.strip()]\n",
    "\n",
    "        # Extract depth and all available variable values dynamically\n",
    "        values = line.split(\",\")\n",
    "        if len(values) > 1 and values[0].strip().isdigit():  # Ensure numeric depth\n",
    "            try:\n",
    "                depth_entry = {}\n",
    "                for i, var in enumerate(variable_names):\n",
    "                    try:\n",
    "                        value = float(values[i + 1].strip()) if \"---\" not in values[i + 1] else None\n",
    "                        depth_entry[var] = value\n",
    "                    except ValueError:\n",
    "                        depth_entry[var] = None  # Handle non-numeric values\n",
    "                depth_data.append(depth_entry)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        # End of a record, process and store it\n",
    "        if line.startswith(\"END OF VARIABLES SECTION\"):\n",
    "            if depth_data:\n",
    "                # Compute averages for all variables dynamically\n",
    "                averages = {}\n",
    "                for var in variable_names:\n",
    "                    valid_values = [entry[var] for entry in depth_data if entry[var] is not None]\n",
    "                    averages[f\"Avg_{var}\"] = sum(valid_values) / len(valid_values) if valid_values else None\n",
    "\n",
    "                # Store processed record\n",
    "                current_record.update(averages)\n",
    "                data.append(current_record.copy())\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if save :\n",
    "        file_path = file_path.replace(\".csv\", \"_processed.csv\")\n",
    "        # Save the processed data\n",
    "        df.to_csv(file_path, index=False)\n",
    "    # return df\n",
    "\n",
    "# Uncomment to run the function\n",
    "# prep_data(file_path, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
